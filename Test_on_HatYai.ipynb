{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb33fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %python3 -m pip install -r requirements.txt\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_name = \"Hat Yai District, Songkhla, Thailand\"\n",
    "def load_graph():\n",
    "    return ox.graph_from_place(place_name, network_type='drive')\n",
    "\n",
    "# Build full graph from place\n",
    "graph = load_graph()\n",
    "\n",
    "# Polygon filter\n",
    "area_coordinates = [\n",
    "    (100.502769, 7.037112),\n",
    "    (100.506839, 7.036403),\n",
    "    (100.484659, 6.981756),\n",
    "    (100.483150, 6.987053),\n",
    "    (100.467264, 6.986236),\n",
    "    (100.466289, 6.983138),\n",
    "    (100.461562, 6.984726),\n",
    "    (100.460087, 6.997991),\n",
    "    (100.451423, 7.006697),\n",
    "    (100.450599, 7.022179),\n",
    "    (100.452207, 7.026726),\n",
    "    (100.477673, 7.027629),\n",
    "    (100.478553, 7.030400),\n",
    "    (100.481593, 7.031277),\n",
    "    (100.484508, 7.017035),\n",
    "    (100.496636, 7.018713),\n",
    "]\n",
    "polygon = Polygon(area_coordinates)\n",
    "\n",
    "# Clip graph to polygon\n",
    "graph_filtered = ox.truncate.truncate_graph_polygon(graph, polygon)\n",
    "\n",
    "# Plot\n",
    "fig, ax = ox.plot_graph(graph_filtered, figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da69a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = ox.graph_to_gdfs(graph_filtered, nodes=False, edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da61668",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e71638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nodes and edges as GeoDataFrames\n",
    "nodes, edges = ox.graph_to_gdfs(graph_filtered)\n",
    "\n",
    "print(f\"Network Statistics:\")\n",
    "print(f\"Number of nodes: {len(nodes)}\")\n",
    "print(f\"Number of edges: {len(edges)}\")\n",
    "print(f\"\\nNodes columns: {list(nodes.columns)}\")\n",
    "print(f\"\\nEdges columns: {list(edges.columns)}\")\n",
    "\n",
    "# Check the bounds of the network\n",
    "bounds = nodes.total_bounds\n",
    "print(f\"\\nNetwork bounds (lon_min, lat_min, lon_max, lat_max): {bounds}\")\n",
    "print(edges['length'])\n",
    "# Export nodes and edges to CSV files\n",
    "nodes_csv = nodes.copy()\n",
    "# Convert geometry to separate lat/lon columns for CSV export\n",
    "nodes_csv['longitude'] = nodes_csv.geometry.x\n",
    "nodes_csv['latitude'] = nodes_csv.geometry.y\n",
    "nodes_csv = nodes_csv.drop('geometry', axis=1)\n",
    "\n",
    "edges_csv = edges.copy()\n",
    "# Drop geometry column for CSV export as it's complex\n",
    "edges_csv = edges_csv.drop('geometry', axis=1)\n",
    "\n",
    "# Export to CSV\n",
    "nodes_csv.to_csv('data/raw/road/hatyai_nodes.csv', index=True)\n",
    "edges_csv.to_csv('data/raw/road/hatyai_edges.csv', index=True)\n",
    "\n",
    "print(f\"\\nExported data:\")\n",
    "print(f\"Nodes exported to 'hatyai_nodes.csv' ({len(nodes_csv)} records)\")\n",
    "print(f\"Edges exported to 'hatyai_edges.csv' ({len(edges_csv)} records)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace71efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv('data/raw/road/hatyai_nodes.csv')\n",
    "edges = pd.read_csv('data/raw/road/hatyai_edges.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10842505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edge/node capacities using length and lanes (default lane width 3.5 m)\n",
    "\n",
    "def add_capacity(nodes_df, edges_df, lane_width=3.5, default_lanes=1):\n",
    "\n",
    "    edges_df = edges_df.copy()\n",
    "\n",
    "    nodes_df = nodes_df.copy()\n",
    "\n",
    "    lanes_numeric = pd.to_numeric(edges_df.get(\"lanes\"), errors=\"coerce\")\n",
    "\n",
    "    lanes_numeric = lanes_numeric.fillna(default_lanes).clip(lower=default_lanes)\n",
    "\n",
    "    edges_df[\"lanes_clean\"] = lanes_numeric\n",
    "\n",
    "    edges_df[\"capacity\"] = edges_df[\"length\"].fillna(0) * lane_width * edges_df[\"lanes_clean\"]\n",
    "\n",
    "    node_capacity = (\n",
    "\n",
    "        pd.concat(\n",
    "\n",
    "            [\n",
    "\n",
    "                edges_df[[\"u\", \"capacity\"]].rename(columns={\"u\": \"osmid\"}),\n",
    "\n",
    "                edges_df[[\"v\", \"capacity\"]].rename(columns={\"v\": \"osmid\"}),\n",
    "\n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "        .groupby(\"osmid\")[\"capacity\"]\n",
    "\n",
    "        .sum()\n",
    "\n",
    "        .rename(\"capacity\")\n",
    "\n",
    "    )\n",
    "\n",
    "    nodes_df = nodes_df.merge(node_capacity, on=\"osmid\", how=\"left\")\n",
    "\n",
    "    nodes_df[\"capacity\"] = nodes_df[\"capacity\"].fillna(0)\n",
    "\n",
    "    return nodes_df, edges_df\n",
    "\n",
    "\n",
    "\n",
    "nodes, edges = add_capacity(nodes, edges)\n",
    "\n",
    "nodes.to_csv(\"data/clean/hatyai_nodes_with_capacity.csv\", index=False)\n",
    "\n",
    "edges.to_csv(\"data/clean/hatyai_edges_with_capacity.csv\", index=False)\n",
    "\n",
    "print(\n",
    "\n",
    "    \"Capacities added: \"\n",
    "\n",
    "    f\"nodes with capacity={len(nodes)}; \"\n",
    "\n",
    "    f\"edges with capacity={len(edges)}; \"\n",
    "\n",
    "    f\"lane width={3.5}m\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load shelters from text file and save as GeoDataFrame\n",
    "\n",
    "def load_shelters(txt_path=\"data/tmp/โรงเรียนในหาดใหญ่.txt\"):\n",
    "\n",
    "    # Expect lines: name, lat, lon\n",
    "\n",
    "    shelters = pd.read_csv(\n",
    "\n",
    "        txt_path,\n",
    "\n",
    "        header=None,\n",
    "\n",
    "        names=[\"name\", \"lat\", \"lon\"],\n",
    "\n",
    "        sep=\",\",\n",
    "\n",
    "        engine=\"python\",\n",
    "\n",
    "        skipinitialspace=True,\n",
    "\n",
    "    )\n",
    "\n",
    "    shelters[\"lat\"] = shelters[\"lat\"].astype(float)\n",
    "\n",
    "    shelters[\"lon\"] = shelters[\"lon\"].astype(float)\n",
    "\n",
    "    shelters_gdf = gpd.GeoDataFrame(\n",
    "\n",
    "        shelters,\n",
    "\n",
    "        geometry=gpd.points_from_xy(shelters[\"lon\"], shelters[\"lat\"]),\n",
    "\n",
    "        crs=\"EPSG:4326\",\n",
    "\n",
    "    )\n",
    "\n",
    "    shelters_gdf.to_csv(\"data/clean/hatyai_shelters.csv\", index=False)\n",
    "\n",
    "    return shelters_gdf\n",
    "\n",
    "\n",
    "\n",
    "shelters = load_shelters()\n",
    "\n",
    "print(f\"Shelters loaded: {len(shelters)}\")\n",
    "\n",
    "print(shelters.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287cc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach shelters onto nearest road nodes\n",
    "\n",
    "def attach_shelters_to_nodes(nodes_df, shelters_gdf):\n",
    "\n",
    "    nodes_geo = gpd.GeoDataFrame(\n",
    "\n",
    "        nodes_df.copy(),\n",
    "\n",
    "        geometry=gpd.points_from_xy(nodes_df.x, nodes_df.y),\n",
    "\n",
    "        crs=\"EPSG:4326\",\n",
    "\n",
    "    )\n",
    "\n",
    "    nearest = gpd.sjoin_nearest(\n",
    "\n",
    "        shelters_gdf,\n",
    "\n",
    "        nodes_geo,\n",
    "\n",
    "        how=\"left\",\n",
    "\n",
    "        distance_col=\"shelter_node_dist_deg\",\n",
    "\n",
    "    )\n",
    "\n",
    "    grouped = nearest.groupby(\"index_right\").agg(\n",
    "\n",
    "        shelter_names=(\"name\", lambda s: \"; \".join(sorted(set(s.dropna())))),\n",
    "\n",
    "        shelter_min_dist_deg=(\"shelter_node_dist_deg\", \"min\"),\n",
    "\n",
    "    )\n",
    "\n",
    "    nodes_out = nodes_df.copy()\n",
    "\n",
    "    nodes_out[\"is_shelter\"] = False\n",
    "\n",
    "    nodes_out[\"shelter_names\"] = np.nan\n",
    "\n",
    "    nodes_out[\"shelter_min_dist_deg\"] = np.nan\n",
    "\n",
    "    nodes_out.loc[grouped.index, \"is_shelter\"] = True\n",
    "\n",
    "    nodes_out.loc[grouped.index, \"shelter_names\"] = grouped[\"shelter_names\"].values\n",
    "\n",
    "    nodes_out.loc[grouped.index, \"shelter_min_dist_deg\"] = grouped[\n",
    "\n",
    "        \"shelter_min_dist_deg\"\n",
    "\n",
    "    ].values\n",
    "\n",
    "    return nodes_out\n",
    "\n",
    "\n",
    "\n",
    "nodes = attach_shelters_to_nodes(nodes, shelters)\n",
    "\n",
    "nodes.to_csv(\"data/clean/hatyai_nodes_with_capacity_and_shelter.csv\", index=False)\n",
    "\n",
    "print(\"Shelters attached to nodes:\", nodes[\"is_shelter\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly allocate population/households across nodes\n",
    "pop_male = 64864\n",
    "pop_female = 75987\n",
    "pop_total = 140851\n",
    "households = 70703\n",
    "\n",
    "rng = np.random.default_rng(888)\n",
    "n = len(nodes)\n",
    "weights = np.ones(n) / n\n",
    "\n",
    "nodes[\"pop_male\"] = rng.multinomial(pop_male, weights)\n",
    "nodes[\"pop_female\"] = rng.multinomial(pop_female, weights)\n",
    "nodes[\"pop_total\"] = nodes[\"pop_male\"] + nodes[\"pop_female\"]\n",
    "nodes[\"households\"] = rng.multinomial(households, weights)\n",
    "\n",
    "print(\"Population assigned:\"\n",
    "      f\" male={nodes['pop_male'].sum()}\"\n",
    "      f\" female={nodes['pop_female'].sum()}\"\n",
    "      f\" total={nodes['pop_total'].sum()}\"\n",
    "      f\" households={nodes['households'].sum()}\")\n",
    "\n",
    "nodes.to_csv(\"data/clean/hatyai_nodes_with_pop.csv\", index=False)\n",
    "print(\"Saved with population -> data/clean/hatyai_nodes_with_pop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb28265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load flood grid data for multiple time snapshots (scalable t1, t2, t3, ...)\n",
    "# Configure your snapshots here (label, path, timestamp). Add/remove rows as needed.\n",
    "flood_snapshots = [\n",
    "    {\n",
    "        \"label\": \"s0\",\n",
    "        \"path\": \"data/raw/flood/so.csv\",\n",
    "        \"timestamp\": \"2025-11-13 22:00:00\",\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"s1\",\n",
    "        \"path\": \"data/tmp/filtered_bc5_20251124_2200_locations_inside.csv\",\n",
    "        \"timestamp\": \"2025-11-24 22:00:00\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def load_flood_snapshot(path, label, timestamp):\n",
    "    df = pd.read_csv(path)\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df.x, df.y),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "    return {\"label\": label, \"timestamp\": pd.to_datetime(timestamp), \"gdf\": gdf}\n",
    "\n",
    "\n",
    "snapshots = [load_flood_snapshot(**snap) for snap in flood_snapshots]\n",
    "\n",
    "nodes_gdf = gpd.GeoDataFrame(\n",
    "    nodes.copy(),\n",
    "    geometry=gpd.points_from_xy(nodes.x, nodes.y),\n",
    "    crs=\"EPSG:4326\",\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Keep node table aligned to the GeoDataFrame length/index\n",
    "nodes_with_flood = nodes_gdf.drop(columns=\"geometry\").copy()\n",
    "\n",
    "for snap in snapshots:\n",
    "    label = snap[\"label\"]\n",
    "    distance_col = f\"flood_distance_{label}_deg\"\n",
    "    right_gdf = snap[\"gdf\"][['gridcode', 'geometry']].copy()\n",
    "    joined = gpd.sjoin_nearest(\n",
    "        nodes_gdf,\n",
    "        right_gdf,\n",
    "        how=\"left\",\n",
    "        distance_col=distance_col,\n",
    "    ).rename(columns={\"gridcode\": f\"flood_level_{label}\"})\n",
    "    # Deduplicate on node index (take closest match) to keep length consistent\n",
    "    joined = (\n",
    "        joined.sort_values(distance_col)\n",
    "        .groupby(level=0)\n",
    "        .first()\n",
    "        .reindex(nodes_with_flood.index)\n",
    "    )\n",
    "    nodes_with_flood[f\"flood_level_{label}\"] = joined[f\"flood_level_{label}\"].to_numpy()\n",
    "    nodes_with_flood[distance_col] = joined[distance_col].to_numpy()\n",
    "\n",
    "edges_with_flood = edges.copy()\n",
    "# Propagate each snapshot's flood level to edges via max of endpoints\n",
    "for snap in snapshots:\n",
    "    label = snap[\"label\"]\n",
    "    node_levels = nodes_with_flood.groupby(\"osmid\")[f\"flood_level_{label}\"].max()\n",
    "    u_level = edges_with_flood[\"u\"].map(node_levels)\n",
    "    v_level = edges_with_flood[\"v\"].map(node_levels)\n",
    "    edges_with_flood[f\"flood_level_{label}\"] = pd.concat([u_level, v_level], axis=1).max(axis=1)\n",
    "\n",
    "# Save results\n",
    "nodes_with_flood.to_csv(\"data/clean/hatyai_nodes_flood_multi.csv\", index=False)\n",
    "edges_with_flood.to_csv(\"data/clean/hatyai_edges_flood_multi.csv\", index=False)\n",
    "\n",
    "print(\"Flood snapshots attached (scalable):\")\n",
    "print(\"Snapshot labels:\", [s[\"label\"] for s in snapshots])\n",
    "print(nodes_with_flood.filter(regex=\"flood_level_.*\").head())\n",
    "print(edges_with_flood.filter(regex=\"flood_level_.*\").head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a757f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visuals: flood levels, deltas, and map\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "level_to_depth = {1: 0.5, 2: 1.0, 3: 1.5, 4: 2.0, 5: 2.5}\n",
    "depth_t1 = nodes_with_flood[\"flood_level_s0\"].map(level_to_depth)\n",
    "depth_t2 = nodes_with_flood[\"flood_level_s1\"].map(level_to_depth)\n",
    "delta_depth = depth_t2 - depth_t1\n",
    "\n",
    "depth_t1.hist(ax=axes[0], bins=30)\n",
    "axes[0].set_title(\"Node flood depth s0 (m)\")\n",
    "depth_t2.hist(ax=axes[1], bins=30)\n",
    "axes[1].set_title(\"Node flood depth s1 (m)\")\n",
    "delta_depth.hist(ax=axes[2], bins=30)\n",
    "axes[2].set_title(\"Node flood depth delta (m)\")\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"meters\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc291a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps: separate flood levels (t1, t2) with network, and population colored by size\n",
    "# Map categorical flood levels to depth in meters\n",
    "level_to_depth = {1: 0.5, 2: 1.0, 3: 1.5, 4: 2.0, 5: 2.5}\n",
    "nodes_plot = gpd.GeoDataFrame(\n",
    "    nodes_with_flood.assign(\n",
    "        flood_depth_s0_m=nodes_with_flood[\"flood_level_s0\"].map(level_to_depth),\n",
    "        flood_depth_s1_m=nodes_with_flood[\"flood_level_s1\"].map(level_to_depth),\n",
    "    ),\n",
    "    geometry=gpd.points_from_xy(nodes_with_flood.x, nodes_with_flood.y),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "edges_geom = (\n",
    "    ox.graph_to_gdfs(graph_filtered, nodes=False, edges=True)\n",
    "    .reset_index()[[\"u\", \"v\", \"key\", \"geometry\"]]\n",
    ")\n",
    "\n",
    "# Flood depth maps (meters, fixed marker size) + network lines\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "for ax, col, title in zip(\n",
    "    axes,\n",
    "    [\"flood_depth_s0_m\", \"flood_depth_s1_m\"],\n",
    "    [\"Flood depth s0 (m)\", \"Flood depth s1 (m)\"],\n",
    "):\n",
    "    edges_geom.plot(ax=ax, color=\"#999\", linewidth=0.5, alpha=0.4, label=\"Network\")\n",
    "    nodes_plot.plot(\n",
    "        ax=ax,\n",
    "        column=col,\n",
    "        cmap=\"YlOrRd\",\n",
    "        markersize=10,\n",
    "        alpha=0.8,\n",
    "        legend=True,\n",
    "        legend_kwds={\"label\": \"Flood depth (m)\"},\n",
    "    )\n",
    "    shelters.plot(ax=ax, color=\"blue\", markersize=25, marker=\"^\", label=\"Shelter\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_axis_off()\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Population map (size and color by pop_total) + network lines\n",
    "pop_series = nodes_with_flood.get(\"pop_total\")\n",
    "if pop_series is None:\n",
    "    pop_series = pd.Series(1, index=nodes_with_flood.index)\n",
    "pop_norm = (pop_series - pop_series.min()).clip(lower=1)\n",
    "pop_size = 50 * pop_norm / pop_norm.max()\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "edges_geom.plot(ax=ax, color=\"#999\", linewidth=0.5, alpha=0.4, label=\"Network\")\n",
    "nodes_plot.plot(\n",
    "    ax=ax,\n",
    "    column=pop_series,\n",
    "    cmap=\"Blues\",\n",
    "    markersize=pop_size,\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"k\",\n",
    "    linewidth=0.2,\n",
    "    legend=True,\n",
    "    legend_kwds={\"label\": \"Population\"},\n",
    ")\n",
    "shelters.plot(ax=ax, color=\"red\", markersize=25, marker=\"^\", label=\"Shelter\")\n",
    "ax.set_title(\"Population distribution (size & color)\")\n",
    "ax.set_axis_off()\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70755169",
   "metadata": {},
   "source": [
    "# Flood-aware evacuation parameters and simulation setup\n",
    "\n",
    "This section defines speed/slowdown assumptions, assigns a travel mode per node, builds a time-expanded graph (t1→t2 by default, configurable), computes a max-flow to shelters, and prepares an animation of network availability under flooding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc152eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters: speeds, flood slowdown, time steps, and mode assignment (scalable flood snapshots)\n",
    "import datetime as dt\n",
    "\n",
    "# Base speeds (km/h). You can tweak per run.\n",
    "base_speed_kmh = {\n",
    "    \"walk\": 5.0,   # walking 4–5 km/h\n",
    "    \"drive\": 50.0  # urban driving 40–60 km/h\n",
    "}\n",
    "\n",
    "# Flood slowdown factors by level (multiplicative). Levels >= impassable_level drop the edge.\n",
    "flood_slowdown = {\n",
    "    0: 1.0,   # dry\n",
    "    1: 0.5,\n",
    "    2: 0.2,\n",
    "    3: 0.0,   # level 3+ impassable by rule below\n",
    "    4: 0.0,\n",
    "    5: 0.0,\n",
    "}\n",
    "impassable_level = 3\n",
    "\n",
    "# Time grid: 5-minute steps spanning min/max snapshot timestamps\n",
    "snapshot_labels = [s[\"label\"] for s in snapshots]\n",
    "snapshot_times = [pd.to_datetime(s[\"timestamp\"]) for s in snapshots]\n",
    "snapshot_order = np.argsort(snapshot_times)\n",
    "snapshot_labels = [snapshot_labels[i] for i in snapshot_order]\n",
    "snapshot_times = [snapshot_times[i] for i in snapshot_order]\n",
    "\n",
    "# Use snapshot range unless you want to override manually\n",
    "# t_start = pd.Timestamp(\"2025-11-24 22:00:00\")\n",
    "# t_end = pd.Timestamp(\"2025-11-26 18:00:00\")\n",
    "t_start = snapshot_times[0]\n",
    "t_end = snapshot_times[-1]\n",
    "freq = \"5min\"\n",
    "time_index = pd.date_range(start=t_start, end=t_end, freq=freq)\n",
    "time_steps = [\n",
    "    {\"name\": f\"t{i}\", \"timestamp\": ts}\n",
    "    for i, ts in enumerate(time_index)\n",
    "]\n",
    "\n",
    "# Incremental flood update using interpolation across arbitrary snapshots\n",
    "level_to_depth = {1: 0.5, 2: 1.0, 3: 1.5, 4: 2.0, 5: 2.5}\n",
    "depth_to_level = {v: k for k, v in level_to_depth.items()}\n",
    "depths_sorted = np.array(sorted(depth_to_level.keys()))\n",
    "\n",
    "# Build depth matrix per edge for each snapshot\n",
    "edge_depth_matrix = []\n",
    "for lbl in snapshot_labels:\n",
    "    edge_depth_matrix.append(\n",
    "        edges_with_flood[f\"flood_level_{lbl}\"].map(level_to_depth).fillna(depths_sorted.min()).to_numpy()\n",
    "    )\n",
    "edge_depth_matrix = np.vstack(edge_depth_matrix).T  # shape: (n_edges, n_snapshots)\n",
    "\n",
    "snapshot_hours = np.array([(ts - t_start).total_seconds() / 3600.0 for ts in snapshot_times])\n",
    "\n",
    "# Build all interpolated flood columns at once to avoid fragmentation\n",
    "flood_cols = {}\n",
    "for i, ts in enumerate(time_index):\n",
    "    hour = (ts - t_start).total_seconds() / 3600.0\n",
    "    depth_now = np.apply_along_axis(\n",
    "        lambda row: np.interp(hour, snapshot_hours, row, left=row[0], right=row[-1]),\n",
    "        1,\n",
    "        edge_depth_matrix,\n",
    "    )\n",
    "    nearest_depth = depths_sorted[np.abs(depth_now[:, None] - depths_sorted).argmin(axis=1)]\n",
    "    flood_cols[f\"flood_level_t{i}\"] = pd.Series(nearest_depth).map(depth_to_level).values\n",
    "# Concatenate in one go\n",
    "edges_with_flood = pd.concat([edges_with_flood.reset_index(drop=True), pd.DataFrame(flood_cols)], axis=1)\n",
    "\n",
    "# Mode choice probabilities (editable). Must sum to 1.\n",
    "mode_probs = {\"walk\": 0.8, \"drive\": 0.2}\n",
    "mode_labels = list(mode_probs.keys())\n",
    "mode_weights = np.array(list(mode_probs.values()), dtype=float)\n",
    "mode_weights = mode_weights / mode_weights.sum()\n",
    "\n",
    "rng_modes = np.random.default_rng(6629)\n",
    "\n",
    "nodes_modes = nodes_with_flood.copy()\n",
    "if \"mode\" not in nodes_modes.columns or nodes_modes[\"mode\"].isna().any():\n",
    "    nodes_modes[\"mode\"] = rng_modes.choice(mode_labels, size=len(nodes_modes), p=mode_weights)\n",
    "\n",
    "print(nodes_modes[\"mode\"].value_counts(normalize=True).rename(\"share\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build time-expanded graph and compute max-flow to shelters (nearest-sink via Dijkstra)\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "\n",
    "# Helpers (deduplicate osmid to avoid Series-return on .loc)\n",
    "mode_lookup = nodes_modes.groupby(\"osmid\")[\"mode\"].agg(lambda s: s.iloc[0])\n",
    "pop_lookup = nodes_modes.groupby(\"osmid\")[\"pop_total\"].sum().fillna(0)\n",
    "capacity_lookup = nodes_modes.groupby(\"osmid\")[\"capacity\"].max().fillna(1e6)\n",
    "\n",
    "# Map time names to flood level columns (expects flood_level_<name>)\n",
    "time_names = [t[\"name\"] for t in time_steps]\n",
    "time_stamps = [pd.to_datetime(t[\"timestamp\"]) for t in time_steps]\n",
    "\n",
    "missing_cols = [c for c in [f\"flood_level_{n}\" for n in time_names] if c not in edges_with_flood.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing flood columns in edges_with_flood: {missing_cols}\")\n",
    "\n",
    "# Precompute delta hours between steps\n",
    "step_hours = [\n",
    "    (time_stamps[i + 1] - time_stamps[i]).total_seconds() / 3600.0\n",
    "    for i in range(len(time_stamps) - 1)\n",
    "]\n",
    "\n",
    "\n",
    "def edge_speed_kmh(mode: str, flood_level: float) -> float:\n",
    "    # Driving stops at flood level >= 1 by request\n",
    "    if mode == \"drive\" and flood_level >= 1:\n",
    "        return 0.0\n",
    "    if flood_level >= impassable_level:\n",
    "        return 0.0\n",
    "    factor = flood_slowdown.get(int(flood_level), flood_slowdown.get(0, 1.0))\n",
    "    return base_speed_kmh.get(mode, 0.0) * factor\n",
    "\n",
    "\n",
    "def travel_time_hours(length_m: float, speed_kmh: float) -> float:\n",
    "    if speed_kmh <= 0:\n",
    "        return np.inf\n",
    "    return (length_m / 1000.0) / speed_kmh\n",
    "\n",
    "\n",
    "def build_time_expanded_graph(nodes_df, edges_df):\n",
    "    t0 = time.time()\n",
    "    nodes_unique = nodes_df.drop_duplicates(subset=[\"osmid\"])  \n",
    "    G = nx.DiGraph()\n",
    "    # Layered nodes\n",
    "    for ti, name in enumerate(time_names):\n",
    "        for _, n in nodes_unique.iterrows():\n",
    "            osmid = int(n[\"osmid\"])\n",
    "            node_id = (osmid, ti)\n",
    "            G.add_node(\n",
    "                node_id,\n",
    "                time=name,\n",
    "                mode=mode_lookup.loc[osmid],\n",
    "                is_shelter=bool(n.get(\"is_shelter\", False)),\n",
    "                pop=float(pop_lookup.loc[osmid]),\n",
    "            )\n",
    "    # Waiting edges\n",
    "    for ti in range(len(time_names) - 1):\n",
    "        for _, n in nodes_unique.iterrows():\n",
    "            osmid = int(n[\"osmid\"])\n",
    "            node_id_now = (osmid, ti)\n",
    "            node_id_next = (osmid, ti + 1)\n",
    "            G.add_edge(\n",
    "                node_id_now,\n",
    "                node_id_next,\n",
    "                capacity=float(capacity_lookup.loc[osmid]),\n",
    "                travel_hours=step_hours[ti],\n",
    "                kind=\"wait\",\n",
    "            )\n",
    "    # Movement edges (apply flood slowdown/impassable)\n",
    "    for ti, name in enumerate(time_names[:-1]):\n",
    "        flood_col = f\"flood_level_{name}\"\n",
    "        for _, e in edges_df.iterrows():\n",
    "            flood_level = e.get(flood_col, 0)\n",
    "            if pd.isna(flood_level):\n",
    "                flood_level = 0\n",
    "            mode_u = mode_lookup.get(e[\"u\"], \"walk\")\n",
    "            speed = edge_speed_kmh(mode_u, flood_level)\n",
    "            if speed <= 0:\n",
    "                continue\n",
    "            t_hours = travel_time_hours(e.get(\"length\", 0), speed)\n",
    "            if t_hours > step_hours[ti]:\n",
    "                continue  # cannot arrive within next time layer\n",
    "            G.add_edge(\n",
    "                (int(e[\"u\"]), ti),\n",
    "                (int(e[\"v\"]), ti + 1),\n",
    "                capacity=float(e.get(\"capacity\", 1e6)),\n",
    "                travel_hours=t_hours,\n",
    "                kind=\"move\",\n",
    "                mode=mode_u,\n",
    "                flood_level=float(flood_level),\n",
    "            )\n",
    "    print(f\"Time-expanded graph built in {time.time()-t0:.1f}s: nodes={G.number_of_nodes()} edges={G.number_of_edges()}\")\n",
    "    return G\n",
    "\n",
    "\n",
    "def pick_nearest_shelter_targets(G):\n",
    "    \"\"\"Use Dijkstra travel_hours from each origin (t0) to shelters (last layer) to select closest sink per origin.\n",
    "    Falls back to all shelters if none are reachable.\"\"\"\n",
    "    last_layer = len(time_names) - 1\n",
    "    shelter_targets = [(int(osmid), last_layer) for osmid in nodes_modes.loc[nodes_modes[\"is_shelter\"] == True, \"osmid\"].drop_duplicates()]\n",
    "    if not shelter_targets:\n",
    "        raise ValueError(\"No shelters found for Dijkstra search.\")\n",
    "    origins = [(int(osmid), 0) for osmid in nodes_modes.loc[nodes_modes[\"pop_total\"] > 0, \"osmid\"].drop_duplicates()]\n",
    "    chosen = set()\n",
    "    unreachable = 0\n",
    "    for origin in origins:\n",
    "        dist = nx.single_source_dijkstra_path_length(G, origin, weight=\"travel_hours\")\n",
    "        reachable = {t: dist[t] for t in shelter_targets if t in dist}\n",
    "        if not reachable:\n",
    "            unreachable += 1\n",
    "            continue\n",
    "        best_target = min(reachable, key=reachable.get)\n",
    "        chosen.add(best_target)\n",
    "    if not chosen:\n",
    "        print(\"No shelters reachable via Dijkstra; falling back to all shelters as sinks.\")\n",
    "        chosen = set(shelter_targets)\n",
    "    if unreachable:\n",
    "        print(f\"Origins with no reachable shelter routes: {unreachable}/{len(origins)} (kept reachable ones)\")\n",
    "    # return only osmids (we will connect all time layers for those osmids)\n",
    "    return {t[0] for t in chosen}\n",
    "\n",
    "\n",
    "def run_evacuation_max_flow(G):\n",
    "    H = G.copy()\n",
    "    super_source = \"super_source\"\n",
    "    super_sink = \"super_sink\"\n",
    "    H.add_node(super_source)\n",
    "    H.add_node(super_sink)\n",
    "\n",
    "    total_demand = pop_lookup.sum()\n",
    "    # Inject demand at first layer\n",
    "    for osmid, pop in pop_lookup.items():\n",
    "        if pop <= 0:\n",
    "            continue\n",
    "        H.add_edge(super_source, (int(osmid), 0), capacity=float(pop))\n",
    "\n",
    "    # Pick closest sinks via Dijkstra with fallback (osmids only)\n",
    "    chosen_shelter_osmids = pick_nearest_shelter_targets(G)\n",
    "\n",
    "    # Connect chosen shelters in ALL time layers to allow early arrival\n",
    "    for osmid in chosen_shelter_osmids:\n",
    "        for layer in range(len(time_names)):\n",
    "            H.add_edge((int(osmid), layer), super_sink, capacity=1e9)\n",
    "\n",
    "    print(f\"Running max-flow on graph with {H.number_of_nodes()} nodes and {H.number_of_edges()} edges...\")\n",
    "    flow_value, flow_dict = nx.maximum_flow(H, super_source, super_sink, capacity=\"capacity\")\n",
    "    evac_rate = flow_value / total_demand if total_demand > 0 else 0\n",
    "\n",
    "    # Summaries by shelter\n",
    "    shelter_flow = defaultdict(float)\n",
    "    for osmid in chosen_shelter_osmids:\n",
    "        for layer in range(len(time_names)):\n",
    "            shelter_flow[int(osmid)] += flow_dict.get((int(osmid), layer), {}).get(super_sink, 0)\n",
    "\n",
    "    print(f\"Total demand: {total_demand:.0f} people\")\n",
    "    print(f\"Max-flow evacuated: {flow_value:.0f} people ({evac_rate:.1%} of demand)\")\n",
    "    print(\"Shelter arrivals (people):\")\n",
    "    for osmid, val in shelter_flow.items():\n",
    "        print(f\"  node {osmid}: {val:.0f}\")\n",
    "\n",
    "    return {\n",
    "        \"flow_value\": flow_value,\n",
    "        \"evac_rate\": evac_rate,\n",
    "        \"flow_dict\": flow_dict,\n",
    "        \"graph\": H,\n",
    "        \"shelter_flow\": shelter_flow,\n",
    "    }\n",
    "\n",
    "\n",
    "t0_build = time.time()\n",
    "G_te = build_time_expanded_graph(nodes_modes, edges_with_flood)\n",
    "print(f\"Build finished in {time.time()-t0_build:.1f}s\")\n",
    "flow_results = run_evacuation_max_flow(G_te)\n",
    "print(\"nodes_unique:\", len(nodes_modes.drop_duplicates(\"osmid\")))\n",
    "print(\"time layers:\", len(time_names))\n",
    "print(\"wait edges:\", len(nodes_modes.drop_duplicates(\"osmid\")) * (len(time_names)-1))\n",
    "tmpG = G_te\n",
    "print(\"G nodes:\", tmpG.number_of_nodes(), \"edges:\", tmpG.number_of_edges())\n",
    "move_edges = sum(1 for _,_,d in tmpG.edges(data=True) if d.get(\"kind\") == \"move\")\n",
    "print(\"move edges:\", move_edges)\n",
    "print(\"chosen shelters:\", len(pick_nearest_shelter_targets(tmpG)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation: network availability and evac movement (red dots flowing from sources to sinks)\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Rebuild geometries for animation\n",
    "edges_anim = (\n",
    "    ox.graph_to_gdfs(graph_filtered, nodes=False, edges=True)\n",
    "    .reset_index()[[\"u\", \"v\", \"key\", \"geometry\"]]\n",
    "    .merge(\n",
    "        edges_with_flood[[\"u\", \"v\", \"key\"] + [f\"flood_level_{n}\" for n in time_names]],\n",
    "        on=[\"u\", \"v\", \"key\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "\n",
    "nodes_anim = gpd.GeoDataFrame(\n",
    "    nodes_modes,\n",
    "    geometry=gpd.points_from_xy(nodes_modes.x, nodes_modes.y),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "mode_colors = {\"walk\": \"#1b9e77\", \"drive\": \"#7570b3\"}\n",
    "pop_series = nodes_anim.get(\"pop_total\", pd.Series(1, index=nodes_anim.index))\n",
    "pop_norm = (pop_series - pop_series.min()).clip(lower=1)\n",
    "pop_size = 35 * pop_norm / pop_norm.max()\n",
    "\n",
    "# Precompute edge geometry lookup for movement visualization\n",
    "geom_lookup = {}\n",
    "for _, row in edges_anim.iterrows():\n",
    "    geom_lookup[(int(row.u), int(row.v))] = row.geometry\n",
    "\n",
    "# Build per-frame agent positions from max-flow move edges\n",
    "flow_dict = flow_results[\"flow_dict\"]\n",
    "H_flow = flow_results[\"graph\"]  # includes super source/sink plus move edges\n",
    "agent_scale = 20 # people per animated dot (smaller -> more dots)\n",
    "agent_cap = 200      # max dots per edge per frame\n",
    "agent_positions = [[] for _ in time_names]\n",
    "\n",
    "for (u, v, data) in H_flow.edges(data=True):\n",
    "    if data.get(\"kind\") != \"move\":\n",
    "        continue\n",
    "    flow_val = flow_dict.get(u, {}).get(v, 0)\n",
    "    if flow_val <= 0:\n",
    "        continue\n",
    "    ti = u[1]\n",
    "    if ti >= len(time_names):\n",
    "        continue\n",
    "    geom = geom_lookup.get((int(u[0]), int(v[0]))) or geom_lookup.get((int(v[0]), int(u[0])))\n",
    "    if geom is None:\n",
    "        continue\n",
    "    n_agents = int(min(max(flow_val / agent_scale, 1), agent_cap))\n",
    "    for j in range(n_agents):\n",
    "        frac = (j + 1) / (n_agents + 1)\n",
    "        pt = geom.interpolate(frac, normalized=True)\n",
    "        agent_positions[ti].append((pt.x, pt.y))\n",
    "\n",
    "if sum(len(f) for f in agent_positions) == 0:\n",
    "    print(\"No evac movement to animate (flow is zero on move edges). Check flow_results and parameters.\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "\n",
    "def update(frame_idx):\n",
    "    ax.clear()\n",
    "    time_name = time_names[frame_idx]\n",
    "    time_label = time_stamps[frame_idx]\n",
    "    col = f\"flood_level_{time_name}\"\n",
    "    open_mask = edges_anim[col].fillna(0) < impassable_level\n",
    "    closed_mask = ~open_mask\n",
    "\n",
    "    open_edges = edges_anim[open_mask]\n",
    "    closed_edges = edges_anim[closed_mask]\n",
    "\n",
    "    if len(open_edges):\n",
    "        open_edges.plot(ax=ax, color=\"#4caf50\", linewidth=0.6, alpha=0.8, label=\"Open edge\", aspect=\"auto\")\n",
    "    if len(closed_edges):\n",
    "        closed_edges.plot(ax=ax, color=\"#d32f2f\", linewidth=0.6, alpha=0.5, label=\"Closed edge\", aspect=\"auto\")\n",
    "\n",
    "    nodes_anim.plot(\n",
    "        ax=ax,\n",
    "        color=nodes_anim[\"mode\"].map(mode_colors),\n",
    "        markersize=pop_size,\n",
    "        alpha=0.8,\n",
    "        edgecolor=\"k\",\n",
    "        linewidth=0.2,\n",
    "        label=\"Origins\",\n",
    "    )\n",
    "    nodes_anim[nodes_anim[\"is_shelter\"] == True].plot(\n",
    "        ax=ax,\n",
    "        color=\"#ffb300\",\n",
    "        markersize=60,\n",
    "        marker=\"^\",\n",
    "        edgecolor=\"k\",\n",
    "        linewidth=0.4,\n",
    "        label=\"Shelter\",\n",
    "    )\n",
    "    # Moving evac dots for this frame\n",
    "    if agent_positions[frame_idx]:\n",
    "        xs, ys = zip(*agent_positions[frame_idx])\n",
    "        ax.scatter(xs, ys, color=\"#e53935\", s=35, alpha=0.9, label=\"Evac flow\")\n",
    "    ax.set_title(f\"Network status: {time_name} ({time_label})\")\n",
    "    ax.set_axis_off()\n",
    "    if len(open_edges) or len(closed_edges):\n",
    "        ax.set_aspect(\"equal\", adjustable=\"datalim\")\n",
    "    else:\n",
    "        ax.set_aspect(\"auto\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    uniq = dict(zip(labels, handles))\n",
    "    if uniq:\n",
    "        ax.legend(uniq.values(), uniq.keys(), loc=\"lower left\")\n",
    "    return ax\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=len(time_names), interval=800, repeat=True)\n",
    "\n",
    "# Leave saving commented to avoid errors if no frames\n",
    "ani.save(\"data/clean/evac_animation.gif\", writer=\"pillow\", fps=2)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
